# -*- coding: utf-8 -*-
"""jhanvisharma_review_dl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mogb8nIPc-JPjlLNp4R2ZkGf2N25CUBA
"""

from IPython import get_ipython
import warnings
warnings.filterwarnings("ignore")
ipython = get_ipython()

import torch
import random
import numpy as np

torch.manual_seed(0)
random.seed(0)
np.random.seed(0)
torch.use_deterministic_algorithms(True)

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import os
import re
import warnings

warnings.filterwarnings("ignore")

import pandas as pd
import json

from gensim.models import KeyedVectors

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix

get_ipython().run_line_magic('matplotlib', 'inline')
from matplotlib import pyplot as plt
import seaborn as sns

from gensim.models import KeyedVectors

datatr = "train.txt"
datats = "test.txt"

embeddim = 300

trainset = pd.read_csv(datatr, sep="{}", header=None, usecols=[0,1], names=["review"], encoding="utf8")

testset = pd.read_csv(datats, sep="{}", header=None, usecols=[0,1], names=["review"], encoding="utf8")

def convert_json_object_dict(x):
    newdiction  = json.loads(x.strip())
    return newdiction

jesdata = trainset.apply(lambda x: convert_json_object_dict(x['review']),axis=1)
jes_data = pd.DataFrame(jesdata.tolist())

jtest = testset.apply(lambda x: convert_json_object_dict(x['review']),axis=1)
jtest_data1 = pd.DataFrame(jtest.tolist())

df = jes_data[['reviewText','overall']]

disc = df.rename(columns={'reviewText': 'Review', 'overall': 'Rating'})
disc.head()

type(disc.Rating)

## Running EDA

freqcount = disc.Rating.value_counts()
sns.barplot(x=freqcount.index, y=freqcount.values)

matchword = re.compile(r'[a-z0-9]+') #Declare regex to extract words
numw = disc["Review"].map(lambda x: len(matchword.findall(x.lower())))
graph = sns.boxplot(numw)

## The box plot shows most of the reviews on the left sode with less than 250 words and very few on the right side of the graph

review = np.quantile(numw, 0.90)
print("90th percent quantile of the review length:", review)

maxlength = 259

label = np.array(disc["Rating"])
strait = StratifiedKFold(n_splits=5)
for trn_idx, tst_idx in strait.split(label.reshape((-1, 1)), label):
    break

traindf, testdf = disc.iloc[trn_idx], disc.iloc[tst_idx]

print("Shape of test and train dataframes:", testdf.shape, traindf.shape)

get_ipython().system('pip install wget')
import wget
word2VecFile = os.path.join(os.curdir, 'wiki.multi.en.vec')

if os.path.exists(word2VecFile):
    print('Word2Vec file has been found and is being loaded...')
else:    
    print('Word2Vec file does not exist and needs to be downloaded')
    url = 'https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec'
    wget.download(url)
    print('Downloading from', url)
en_model = KeyedVectors.load_word2vec_format('wiki.multi.en.vec')

en_model = KeyedVectors.load_word2vec_format('wiki.multi.en.vec')

!pip install gensim --upgrade

vocabul = list(en_model.index_to_key)

print("Vocabulary size in pretrained model:", len(vocabul))

# check if the word 'and' is present in the pretrained model
assert "and" in en_model

assert embeddim == len(en_model["and"])

# first row is for the padding token
pretrained_weights = np.zeros((1+len(vocabul), embeddim))

# tqdm just adds a progress bar
for i, token in enumerate(vocabul):
    pretrained_weights[i, :] = en_model[token]

# map tokens in the vocab to ids
vocabul = dict(zip(vocabul, range(1, len(vocabul)+1)))

def reviewText2Features(reviewText):
   
    X = []
    
    wordreview = matchword.findall(reviewText.lower())
    
    for i, word in enumerate(wordreview):
        if word not in en_model:
            continue
        if i >= maxlength:
            break
        X.append(vocabul[word])
    
    if len(X) < maxlength:
        paddingzero = [0.]*(maxlength - len(X))
        X = paddingzero + X
    
    return X
        
def row2Features(row):
    
    X = reviewText2Features(row["Review"])
    y = row["Rating"]
        
    return X, y

rsample = disc.iloc[0]
wordreview = matchword.findall(rsample["Review"].lower())
print("Review:", rsample["Review"])
print("Rating:", rsample["Rating"])
print("Review words:", wordreview)

X, y = row2Features(rsample)
print("Dimensions of X:", len(X))
print("Label y:", y)

def shuffleArray(X, y):
    datax = np.arange(X.shape[0])
    np.random.shuffle(datax)
    X = X[datax, :]
    y = y[datax]
    return X, y

def generateModelReadyData(data, batchsize = 128, shuffle=False):
    
    while(True):
        X = []
        y = []
        for _, row in data.iterrows():
            X_, y_ = row2Features(row)
            X.append(X_)
            y.append(y_)   

            if len(X) > batchsize:
                tempX, tempy = np.array(X[:batchsize]), np.array(y[:batchsize])
                if shuffle:
                    tempX, tempy = shuffleArray(tempX, tempy)
                
                X, y = X[batchsize:], y[batchsize:]                    
                yield tempX, tempy

        if len(X) > 0:
            tempX, tempy = np.array(X), np.array(y)
            if shuffle:
                tempX, tempy = shuffleArray(tempX, tempy)
            
            yield tempX, tempy

disc.info()

numths = 0
for i, (X, y) in enumerate(generateModelReadyData(disc, batchsize=128, shuffle=True)):
    if numths >= 3:
        break
    
    else:
        print("Batch is:", i)
        assert X.shape == (128, maxlength)
        assert y.shape == (128,)
        print("Shapes of X & y matches expected values")
    numths += 1

cudos = torch.cuda.is_available()

if cudos:
    print("cuda available")
    device = torch.device("Cuda")
else:
    device = torch.device("cpu")

torch.manual_seed(0)

class SentimentNet(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, vocab_size, pretrained_weights):
        super(SentimentNet, self).__init__()
        

        self.embedding=nn.Embedding(vocab_size, embedding_dim)
        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weights))
        
        self.sentInputDropout = nn.Dropout(0.3)
        self.biLSTM1 = nn.LSTM(embedding_dim, hidden_dim[0], bidirectional=True, batch_first=True)

        self.biLSTMDropOut = nn.Dropout(0.3)
        self.biLSTM2 = nn.LSTM(2*hidden_dim[0], hidden_dim[1], bidirectional=True, batch_first=True)
        
        self.dropout1 = nn.Dropout(0.3)
        self.dense1 = nn.Linear(2*hidden_dim[1], 75)
        
        self.relu1 = nn.ReLU()
        self.dropout2 = nn.Dropout(0.2)
        
        self.outputLayer = nn.Linear(75, 1)
        self.sigmoid = nn.Sigmoid()
        
        self.hidden_dim = hidden_dim
        
    def forward(self, x):
        
        batch_len = x.shape[0]
        out = self.embedding(x)
        out = self.sentInputDropout(out)
        out, hidden = self.biLSTM1(out)
        out = self.biLSTMDropOut(out)
        out, hidden = self.biLSTM2(out)
        
        out = self.dropout1(out)
        out = self.dense1(out)
        out = self.relu1(out)
        out = self.dropout2(out)
        
        out = self.outputLayer(out)
        out = self.sigmoid(out)
        out = out.view(batch_len, -1)
        out = out[:,-1]
        return out

rwmodel = SentimentNet(embeddim, [259, 128], 1+len(vocabul), pretrained_weights)
rwmodel.to(device)

rwcriteria = nn.BCELoss()
rwoptimizer = torch.optim.Adam(rwmodel.parameters(), lr=0.005)

epochs = 5
rwcount = 0
printall = 1000
rwclip = 5
validmin = np.Inf

rwmodel = rwmodel.float()
rwmodel.train()
for i in range(epochs):
    print("Epoch is:", i+1)
    for j, (inputs, labels) in enumerate(generateModelReadyData(traindf, batchsize=128, shuffle=True)):
        if j >= np.ceil(traindf.shape[0]/128):
            break
        
        rwcount += 1
        inputs, labels = torch.from_numpy(inputs), torch.from_numpy(labels)
        inputs, labels = inputs.to(device), labels.to(device)
        rwmodel.zero_grad()
       
        output = rwmodel(inputs.long())
        
        rwloss = rwcriteria(output.squeeze(), labels.float())
        rwloss.backward()
        nn.utils.clip_grad_norm_(rwmodel.parameters(), rwclip)
        rwoptimizer.step()        
        if (j+1) % 100 == 0:
            print("Batches completed are:", j+1)
    
    print("Batches completed are:", j+1)


    #val_h = model.init_hidden(batch_size)
    valuelosses = []
    rwmodel.eval()
    
    for k, (inp, lab) in enumerate(generateModelReadyData(testdf, batchsize=128, shuffle=False)):
        if k >= np.ceil(testdf.shape[0]/128):
            break
 
        inp, lab = torch.from_numpy(inp), torch.from_numpy(lab)
        inp, lab = inp.to(device), lab.to(device)
        out = rwmodel(inp.long())
        vallos = rwcriteria(out.squeeze(), lab.float())
        valuelosses.append(vallos.item())
        if (k+1) % 100 == 0:
            print("Batches completed are:", k+1)
    
    print("Batches completed are:", k+1)

    rwmodel.train()
    print("Epochs: {}/{}...".format(i+1, epochs),
          "Step: {}...".format(rwcount),
          "Loss: {:.6f}...".format(rwloss.item()),
          "Val Loss: {:.6f}".format(np.mean(valuelosses)))
    if np.mean(valuelosses) <= validmin:
        torch.save(rwmodel.state_dict(), './state_dict.pt')
        print('Validation loss is decreased ({:.6f} --> {:.6f}).'.format(validmin,np.mean(valuelosses)))
        validmin = np.mean(valuelosses)



## Evaluation of the model

losstest = []
corrnum = 0
probprd = []
actualnum = []

rwmodel.eval()
for j, (X_test, y_test) in enumerate(generateModelReadyData(testdf, batchsize=128)):
    if j >= np.ceil(testdf.shape[0]/128):
        break
    
    inputs_test, labels_test = torch.from_numpy(X_test), torch.from_numpy(y_test)
    inputs_test, labels_test = inputs_test.to(device), labels_test.to(device)
    output_test = rwmodel(inputs_test.long())
    losstest2 = rwcriteria(output_test.squeeze(), labels_test.float())
    losstest.append(losstest2.item())
    predict = torch.round(output_test.squeeze())  
    correcttensor = predict.eq(labels_test.float().view_as(predict))
    correct22 = np.squeeze(correcttensor.cpu().numpy())
    corrnum += np.sum(correct22)
    probprd.extend(output_test.cpu().squeeze().detach().numpy())
    actualnum.extend(y_test)
    
    if (j+1) % 100 == 0:
        print("Batches completed are:", j+1)

print("Batches completed are:", j+1)

print("Test loss: {:.3f}".format(np.mean(losstest)))
testaccur = corrnum/len(testdf)
print("Test accuracy: {:.3f}%".format(testaccur*100))

